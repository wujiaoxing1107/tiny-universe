<div align=center>
    <h1><strong>Tiny-LLM + Tiny-RAG built from scratch without using extra tools</strong></h1>
</div>
&emsp;&emsp;This project represents a derivative effort that has been forked from the Tiny Universe initiative. In an endeavor to gain a profound understanding of the underlying working principles and intricate processes governing the model, I undertook a comprehensive local refactoring of the majority of the project code. Leveraging the detailed project structure diagram and meticulously examining the existing codebase, I systematically redesigned and optimized the code, despite the fact that the original code within the project already exhibited a remarkable level of excellence and seemingly left little room for further enhancement. During this process, a notable addition was made to incorporate the option of leveraging KV Cache inference during the inference stage. This enhancement not only augments the efficiency of the inference process but also provides an opportunity to explore advanced techniques in the realm of model deployment and execution, thereby facilitating a more in-depth exploration of the model's capabilities and potential applications.
